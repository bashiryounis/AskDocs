{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up RAG Chain with Memory Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Install Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# ! pip install langchain_openai langchain_community langchain_weaviate redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress specific warning types\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Set environment variables for API keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Processing and Vector Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import weaviate\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate client\n",
    "weaviate_client = weaviate.Client(\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Clean up schema\n",
    "weaviate_client.schema.delete_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data.pdf', 'page': 0}, page_content='GPUSimilarity: \\nSimilarity Searching a Billion Compounds in Real Time \\nPat Lorton \\nhttps://github.com/schrodinger/gpusimilarity'),\n",
       " Document(metadata={'source': './data.pdf', 'page': 1}, page_content='Original Idea, and some questions \\nOriginal idea: \\nComputational complexity of fingerprint similarity (tanimoto) is trivial, \\nembarrassingly parallel, and involves no branching.  Limiting factor of a \\nstraightforward brute force solution is compute power:  This sounds like \\nsomething GPUs would be good at. \\nThen I asked some questions.. \\n2'),\n",
       " Document(metadata={'source': './data.pdf', 'page': 2}, page_content='Idea for architecture \\nThis problem has been solved in different ways with various success using \\ncomplex architectures involving intelligent chemical understanding.  This was \\nrequired because a brute force attempt was simply too expensive in terms of \\nRAM and compute requirements \\nRe-examining this in 2018, RAM and compute costs have continued to plummet, \\nan architecture around brute-force may now be possible (and preferable). \\nReminder:  1 Tesla V100 has a theoretical speed of 14 terahertz \\n                  128 gigs of very fast ram is purchasable for $1000 \\n3')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PDF and split into documents\n",
    "loader = PyPDFLoader(\"./data.pdf\")\n",
    "docs = loader.load_and_split()\n",
    "docs[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.048951830714941025, -0.03986202925443649, -0.021562786772847176]\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceHubEmbeddings()\n",
    "\n",
    "# Generate embeddings for a test document\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "print(query_result[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'page': 0.0, 'source': './data.pdf'}, page_content='GPUSimilarity: \\nSimilarity Searching a Billion Compounds in Real Time \\nPat Lorton \\nhttps://github.com/schrodinger/gpusimilarity'),\n",
       "  1.0),\n",
       " (Document(metadata={'page': 1.0, 'source': './data.pdf'}, page_content='Original Idea, and some questions \\nOriginal idea: \\nComputational complexity of fingerprint similarity (tanimoto) is trivial, \\nembarrassingly parallel, and involves no branching.  Limiting factor of a \\nstraightforward brute force solution is compute power:  This sounds like \\nsomething GPUs would be good at. \\nThen I asked some questions.. \\n2'),\n",
       "  0.6986278295516968),\n",
       " (Document(metadata={'page': 17.0, 'source': './data.pdf'}, page_content='Conclusions \\n•Compute speeds and RAM prices have come down enough that brute-force \\nis a reasonable method for a similarity search server.  It will only become \\ncheaper. \\n•Fingerprint folding with re-scoring is a reasonable method to deal with lower \\nGPU memory than CPU memory. \\n•It’s a race to see whether GPU’s can add more memory faster than \\neasily-purchasable compound libraries will grow. \\n18'),\n",
       "  0.6457107067108154)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# Initialize vector store with Weaviate\n",
    "db = WeaviateVectorStore.from_documents(docs, embeddings, client=client)\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"Similarity\"\n",
    "docs = db.similarity_search(query)\n",
    "docs\n",
    "\n",
    "# Search with score\n",
    "docs = db.similarity_search_with_score(query=query, k=3)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) Chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Define the RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPUSimilarity Searching allows for similarity searching of a billion compounds in real-time using GPUs. The method involves folding fingerprints, merging and sorting results of chunk searches on the GPU, and utilizing brute force with better math for result retrieval. The approach takes advantage of decreasing compute speeds and RAM prices, making brute-force a viable option for similarity search servers.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the RAG chain\n",
    "rag_chain.invoke(\"explain the GPUSimilarity Searching in detail\")\n",
    "\n",
    "def rag(question):\n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "rag(\"explain the GPUSimilarity Searching in detail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management with Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try adding Memeory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Redis for Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIS_URL = \"redis://localhost:6379\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human: Could you please let me know about fingerprints and how GPUsimilarity manages memory constraints',\n",
       " 'ai: Fingerprints are generated using RDKit and stored in memory for similarity searches. To manage memory constraints, the idea of folding fingerprints to fit GPU memory and then re-screening results on the CPU is used. This technique allows for efficient processing while dealing with GPU memory limitations.',\n",
       " 'human: Could you please let me know about fingerprints and how GPUsimilarity manages memory constraints',\n",
       " 'ai: Fingerprints are generated using RDKit and stored in memory for similarity searches. To manage memory constraints, the approach involves folding fingerprints to fit GPU memory and then re-screening results on the CPU. This method helps optimize processing while addressing limitations in GPU memory capacity.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_message_history(user_id: str, conversation_id: str) -> RedisChatMessageHistory:\n",
    "    return RedisChatMessageHistory(session_id=f\"{user_id}:{conversation_id}\", url=REDIS_URL)\n",
    "\n",
    "def get_all_conversations(user_id: str, conversation_id: str) -> list:\n",
    "    chat_history = get_message_history(user_id, conversation_id)\n",
    "    conversation_output = []\n",
    "    for message in chat_history.messages:\n",
    "        conversation_output.append(f\"{message.type}: {message.content}\")\n",
    "    return conversation_output\n",
    "\n",
    "get_all_conversations(user_id=\"666\", conversation_id=\"1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced RAG Chain with Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup History-Aware RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import RedisChatMessageHistory\n",
    "from typing import Optional\n",
    "import bs4\n",
    "import redis\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define contextualization prompt\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create history-aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "# Define QA prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define conversational RAG chain with memory\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"conversation ID\",\n",
    "            description=\"Unique identifier for the session.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous message discussed the challenges faced when trying to scale beyond a \"toy\" case of 20 million compounds. It mentioned issues such as the inability to fit everything inside a GPU's memory, difficulties in allocating multi-gigabyte contiguous blocks of RAM, and limitations in QByteArray's capacity. The message also touched upon the complexity of parallelizing contiguous arrays across multiple GPUs and the solution of slicing memory into chunks during database creation.\n",
      "Fingerprints are generated using RDKit and stored in memory for similarity searches. To address memory constraints, GPUsimilarity folds fingerprints to fit GPU memory and then re-screens results on the CPU. This strategy optimizes processing efficiency while working within the limitations of GPU memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example usage of the conversational RAG chain\n",
    "user_id = \"66b521e88d4963b1300eb61e\"\n",
    "session_id = \"2e1d9dbb-230b-4689-b879-935d18244acc\"\n",
    "\n",
    "response1 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"let me know what the previous message was about\"},\n",
    "    config={\"configurable\": {\"user_id\": user_id, \"conversation_id\": session_id}}\n",
    ")\n",
    "print(response1[\"answer\"])\n",
    "\n",
    "response2 = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Could you please let me know about fingerprints and how GPUsimilarity manages memory constraints\"},\n",
    "    config={\"configurable\": {\"user_id\": \"666\", \"conversation_id\": \"1\"}}\n",
    ")\n",
    "print(response2[\"answer\"])\n",
    "\n",
    "# Print all conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human: Could you please let me know about fingerprints and how GPUsimilarity manages memory constraints',\n",
       " 'ai: Fingerprints are generated using RDKit and stored in memory for similarity searches. To manage memory constraints, the idea of folding fingerprints to fit GPU memory and then re-screening results on the CPU is used. This technique allows for efficient processing while dealing with GPU memory limitations.',\n",
       " 'human: Could you please let me know about fingerprints and how GPUsimilarity manages memory constraints',\n",
       " 'ai: Fingerprints are generated using RDKit and stored in memory for similarity searches. To manage memory constraints, the approach involves folding fingerprints to fit GPU memory and then re-screening results on the CPU. This method helps optimize processing while addressing limitations in GPU memory capacity.',\n",
       " 'human: Could you please let me know about fingerprints and how GPUsimilarity manages memory constraints',\n",
       " 'ai: Fingerprints are generated using RDKit and stored in memory for similarity searches. To address memory constraints, GPUsimilarity folds fingerprints to fit GPU memory and then re-screens results on the CPU. This strategy optimizes processing efficiency while working within the limitations of GPU memory.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_conversations(user_id=\"666\", conversation_id=\"1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
